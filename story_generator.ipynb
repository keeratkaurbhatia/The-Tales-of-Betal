{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Story Generator\n\nThis notebook generates a narrated video from a short story by:\n- Generating narration audio (gTTS)\n- Generating scene clips (Hugging Face T2V API)\n- Stitching with transitions and audio (moviepy)\n- Exporting a small JSON payload for a web client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
                "# Setup\nimport os, json, time, requests\nfrom pathlib import Path\nfrom typing import List\nfrom moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\nfrom gtts import gTTS\n\n# Config\nHUGGINGFACE_TOKEN = \"your_huggingface_token_here\"\nOUTPUT_DIR = Path('generated_content')\nAUDIO_DIR = OUTPUT_DIR / 'audio'\nVIDEOS_DIR = OUTPUT_DIR / 'videos'\nfor d in [OUTPUT_DIR, AUDIO_DIR, VIDEOS_DIR, VIDEOS_DIR / 'clips']:\n    d.mkdir(parents=True, exist_ok=True)\n\n# Story data\nstory_data = {\n    'id': 'wisdom-1',\n    'title': 'The Wise Old Man and the Three Sons',\n    'content': \"\"\"Once upon a time, in a small village, there lived an old man with three sons.\nThe old man was known throughout the village for his wisdom and kindness.\nAs he grew older, he wanted to test which of his sons would inherit his wisdom.\nHe gave each son a single grain of rice and told them to make it multiply by the next full moon.\nThe first son planted it and got a small harvest.\nThe second son sold it and bought more rice.\nBut the third son gave it to a hungry child, saying that kindness multiplies in ways grain cannot.\"\"\",\n    'scenes': [\n        'An old wise man in traditional Indian clothing in a village setting',\n        'Three sons receiving grains of rice from their father',\n        'First son planting rice in a field, traditional Indian farming',\n        'Second son at a marketplace exchanging rice for money',\n        'Third son giving rice to a hungry child, showing compassion',\n        'The wise old man smiling, understanding true wisdom'\n    ]\n}\n\n# TTS (male Indian accent via gTTS + slight pitch drop)\ndef generate_audio_gtts(text: str, filename: str, lang: str = 'en', tld: str = 'co.in') -> Path:\n    out_path = AUDIO_DIR / f'{filename}.mp3'\n    tts = gTTS(text=text, lang=lang, tld=tld, slow=True)\n    tts.save(str(out_path))\n    try:\n        from pydub import AudioSegment\n        from pydub.effects import normalize\n        audio = AudioSegment.from_mp3(str(out_path))\n        audio = audio._spawn(audio.raw_data, overrides={'frame_rate': int(audio.frame_rate * 0.9)})\n        audio = audio.set_frame_rate(44100)\n        audio = normalize(audio)\n        audio.export(str(out_path), format='mp3')\n    except Exception as e:\n        print('pydub post-processing skipped or failed:', e)\n    return out_path\n\n# Direct Text-to-Video API\nHF_T2V_MODEL = 'damo-vilab/text-to-video-ms-1.7b'\nHF_T2V_URL = f'https://api-inference.huggingface.co/models/{HF_T2V_MODEL}'\n\ndef generate_video_clip_hf(prompt: str, filename: str, num_frames: int = 48, fps: int = 24, width: int = 512, height: int = 288) -> Path:\n    headers = {'Authorization': f'Bearer {HUGGINGFACE_TOKEN}', 'Accept': 'video/mp4'}\n    payload = {'inputs': f\"{prompt}, cinematic, Indian cultural aesthetics, vibrant colors, detailed\", 'parameters': {'num_frames': num_frames, 'fps': fps, 'width': width, 'height': height}}\n    clips_dir = VIDEOS_DIR / 'clips'\n    clips_dir.mkdir(exist_ok=True)\n    out_path = clips_dir / f'{filename}.mp4'\n    resp = requests.post(HF_T2V_URL, headers=headers, json=payload, timeout=600)\n    if resp.status_code == 200:\n        with open(out_path, 'wb') as f:\n            f.write(resp.content)\n        return out_path\n    else:\n        raise RuntimeError(f'HF T2V error {resp.status_code}: {resp.text[:200]}')\n\n\ndef generate_scene_clips_from_prompts(story: dict, fps: int = 24, seconds_per_scene: float = 4.0) -> List[str]:\n    clip_paths: List[str] = []\n    num_frames = int(fps * seconds_per_scene)\n    for i, scene_prompt in enumerate(story.get('scenes', []), start=1):\n        fname = f\"{story['id']}_scene_{i:02d}\"\n        p = generate_video_clip_hf(scene_prompt, fname, num_frames=num_frames, fps=fps)\n        clip_paths.append(str(p))\n        time.sleep(2)\n    return clip_paths\n\n# Stitching with audio\ndef stitch_clips_with_audio(clip_paths: List[str], audio_path: Path, story_id: str) -> Path:\n    audio_clip = AudioFileClip(str(audio_path))\n    total_duration = max(1.0, audio_clip.duration)\n    if not clip_paths:\n        raise ValueError('No clips to stitch')\n    duration_per_clip = total_duration / len(clip_paths)\n    transition = 0.4\n    clips = []\n    for i, p in enumerate(clip_paths):\n        c = VideoFileClip(p)\n        c = c.set_duration(duration_per_clip + (transition if i < len(clip_paths)-1 else 0))\n        if i > 0:\n            c = c.crossfadein(transition)\n        clips.append(c)\n    final = concatenate_videoclips(clips, method='compose')\n    final = final.set_duration(total_duration).set_audio(audio_clip).set_fps(24)\n    out_path = VIDEOS_DIR / f'{story_id}_video.mp4'\n    final.write_videofile(str(out_path), codec='libx264', audio_codec='aac', fps=24, verbose=False, logger=None)\n    return out_path\n\n# Subtitles\ndef generate_subtitles(story_content: str, audio_duration: float):\n    sentences = story_content.replace('\\n', ' ').split('. ')\n    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n    if not sentences:\n        return []\n    dur_per = audio_duration / len(sentences)\n    subs, t = [], 0.0\n    for s in sentences:\n        subs.append({'start': round(t, 2), 'end': round(t + dur_per, 2), 'text': s})\n        t += dur_per\n    return subs\n\n# Run pipeline\nnarration_text = story_data['content'].replace('\\n', ' ').strip()\naudio_filename = f\"{story_data['id']}_narration\"\naudio_path = generate_audio_gtts(narration_text, audio_filename)\nclip_paths = generate_scene_clips_from_prompts(story_data, fps=24, seconds_per_scene=4.0)\nvideo_path = stitch_clips_with_audio(clip_paths, audio_path, story_data['id'])\n\n# Export JSON\naudio_clip = AudioFileClip(str(audio_path))\nsubtitles = generate_subtitles(story_data['content'], audio_clip.duration)\nweb_story_data = {\n    'id': story_data['id'],\n    'title': story_data['title'],\n    'content': story_data['content'],\n    'images': [],\n    'audioUrl': f\"/generated_content/audio/{Path(audio_path).name}\",\n    'videoUrl': f\"/generated_content/videos/{Path(video_path).name}\",\n    'subtitles': subtitles\n}\nout_json = OUTPUT_DIR / f\"{story_data['id']}_data.json\"\nwith open(out_json, 'w') as f:\n    json.dump(web_story_data, f, indent=2)\nprint('Done:', video_path, audio_path, out_json)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
