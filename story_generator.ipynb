{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Cultural Storyteller - AI Content Generation\n",
    "\n",
    "This notebook generates images and videos for the Betal storytelling project using free AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests pillow opencv-python moviepy huggingface-hub transformers torch diffusers\n",
    "!pip install elevenlabs gTTS  # For text-to-speech\n",
    "!pip install gradio  # For easy UI testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from gtts import gTTS\n",
    "import io\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys - Replace with your actual keys\n",
    "HUGGINGFACE_TOKEN = \"your_huggingface_token_here\"\n",
    "ELEVENLABS_API_KEY = \"your_elevenlabs_key_here\"  # Optional, can use gTTS instead\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"generated_content\")\n",
    "IMAGES_DIR = OUTPUT_DIR / \"images\"\n",
    "AUDIO_DIR = OUTPUT_DIR / \"audio\"\n",
    "VIDEOS_DIR = OUTPUT_DIR / \"videos\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [IMAGES_DIR, AUDIO_DIR, VIDEOS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample story for generation\n",
    "story_data = {\n",
    "    \"id\": \"wisdom-1\",\n",
    "    \"title\": \"The Wise Old Man and the Three Sons\",\n",
    "    \"content\": \"\"\"Once upon a time, in a small village, there lived an old man with three sons. \n",
    "    The old man was known throughout the village for his wisdom and kindness. \n",
    "    As he grew older, he wanted to test which of his sons would inherit his wisdom.\n",
    "    He gave each son a single grain of rice and told them to make it multiply by the next full moon.\n",
    "    The first son planted it and got a small harvest. \n",
    "    The second son sold it and bought more rice. \n",
    "    But the third son gave it to a hungry child, saying that kindness multiplies in ways grain cannot.\"\"\",\n",
    "    \"scenes\": [\n",
    "        \"An old wise man in traditional Indian clothing in a village setting\",\n",
    "        \"Three sons receiving grains of rice from their father\",\n",
    "        \"First son planting rice in a field, traditional Indian farming\",\n",
    "        \"Second son at a marketplace exchanging rice for money\", \n",
    "        \"Third son giving rice to a hungry child, showing compassion\",\n",
    "        \"The wise old man smiling, understanding true wisdom\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation using Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stable Diffusion pipeline (free model)\n",
    "def initialize_sd_pipeline():\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    \n",
    "    # Use CPU if no GPU available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        pipe.enable_memory_efficient_attention()\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "# Alternative: Use Hugging Face Inference API (free tier)\n",
    "def generate_image_hf_api(prompt, filename):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
    "    \n",
    "    # Enhanced prompt for Indian cultural context\n",
    "    enhanced_prompt = f\"{prompt}, Indian art style, traditional, cultural, detailed, beautiful colors, cinematic lighting\"\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": enhanced_prompt,\n",
    "        \"parameters\": {\n",
    "            \"guidance_scale\": 7.5,\n",
    "            \"num_inference_steps\": 25\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        image_path = IMAGES_DIR / f\"{filename}.png\"\n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Generated image: {image_path}\")\n",
    "        return image_path\n",
    "    else:\n",
    "        print(f\"Error generating image: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images for the story\n",
    "def generate_story_images(story_data):\n",
    "    image_paths = []\n",
    "    \n",
    "    for i, scene_prompt in enumerate(story_data[\"scenes\"]):\n",
    "        filename = f\"{story_data['id']}_scene_{i+1}\"\n",
    "        image_path = generate_image_hf_api(scene_prompt, filename)\n",
    "        \n",
    "        if image_path:\n",
    "            image_paths.append(str(image_path))\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "# Generate images\n",
    "print(\"Generating story images...\")\n",
    "image_paths = generate_story_images(story_data)\n",
    "print(f\"Generated {len(image_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Generation (Text-to-Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use gTTS (Free, supports Indian English accent)\n",
    "def generate_audio_gtts(text, filename, lang='en', tld='co.in'):\n",
     "    \"\"\"Generate audio using Google Text-to-Speech with Indian accent (male-sounding)\"\"\"\n",
     "    tts = gTTS(text=text, lang=lang, tld=tld, slow=True)  # Slower speech for deeper tone\n",
    "    audio_path = AUDIO_DIR / f\"{filename}.mp3\"\n",
    "    tts.save(str(audio_path))\n",
    "    print(f\"Generated audio: {audio_path}\")\n",
    "    return audio_path\n",
    "\n",
    "# Option 2: ElevenLabs API (Better quality, limited free tier)\n",
    "def generate_audio_elevenlabs(text, filename):\n",
     "    \"\"\"Generate audio using ElevenLabs API with Daniel's deep male voice\"\"\"\n",
     "    url = \"https://api.elevenlabs.io/v1/text-to-speech/onwK4e9ZLuTAKqWW03F9\"  # Daniel - mature male voice\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": ELEVENLABS_API_KEY\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        audio_path = AUDIO_DIR / f\"{filename}.mp3\"\n",
    "        with open(audio_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Generated audio: {audio_path}\")\n",
    "        return audio_path\n",
    "    else:\n",
    "        print(f\"Error generating audio: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate narration audio\n",
    "narration_text = story_data[\"content\"].replace('\\n', ' ').strip()\n",
    "audio_filename = f\"{story_data['id']}_narration\"\n",
    "\n",
    "print(\"Generating narration audio...\")\n",
    "# Use gTTS for free option\n",
    "audio_path = generate_audio_gtts(narration_text, audio_filename)\n",
    "\n",
    "# Uncomment below to use ElevenLabs instead\n",
    "# audio_path = generate_audio_elevenlabs(narration_text, audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Generation with Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_parallax_effect(image_path, duration=5):\n",
    "    \"\"\"Add parallax/ken burns effect to image\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize image to be larger for parallax effect\n",
    "    original_size = image.size\n",
    "    new_width = int(original_size[0] * 1.2)\n",
    "    new_height = int(original_size[1] * 1.2)\n",
    "    image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Create image clip with zoom and pan effect\n",
    "    clip = ImageClip(np.array(image), duration=duration)\n",
    "    \n",
    "    # Add zoom effect\n",
    "    clip = clip.resize(lambda t: 1 + 0.1 * t / duration)\n",
    "    \n",
    "    # Add subtle pan effect\n",
    "    clip = clip.set_position(lambda t: ('center', 'center'))\n",
    "    \n",
    "    return clip\n",
    "\n",
    "def add_mystical_effects(clip):\n",
    "    \"\"\"Add mystical visual effects\"\"\"\n",
    "    # Add fade in/out\n",
    "    clip = clip.fadeout(0.5).fadein(0.5)\n",
    "    \n",
    "    # Add subtle color enhancement\n",
    "    clip = clip.fx(afx.colorx, factor=1.1)\n",
    "    \n",
    "    return clip\n",
    "\n",
    "def create_story_video(image_paths, audio_path, story_id):\n",
    "    \"\"\"Create video from images and audio with effects\"\"\"\n",
    "    \n",
    "    # Load audio to get duration\n",
    "    audio_clip = AudioFileClip(str(audio_path))\n",
    "    total_duration = audio_clip.duration\n",
    "    \n",
    "    # Calculate duration per image\n",
    "    duration_per_image = total_duration / len(image_paths)\n",
    "    \n",
    "    # Create video clips from images\n",
    "    video_clips = []\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        if os.path.exists(image_path):\n",
    "            # Add parallax effect\n",
    "            clip = add_parallax_effect(image_path, duration_per_image)\n",
    "            \n",
    "            # Add mystical effects\n",
    "            clip = add_mystical_effects(clip)\n",
    "            \n",
    "            # Set timing\n",
    "            clip = clip.set_start(i * duration_per_image)\n",
    "            \n",
    "            video_clips.append(clip)\n",
    "        else:\n",
    "            print(f\"Image not found: {image_path}\")\n",
    "    \n",
    "    if not video_clips:\n",
    "        print(\"No valid images found for video creation\")\n",
    "        return None\n",
    "    \n",
    "    # Concatenate video clips\n",
    "    final_video = concatenate_videoclips(video_clips, method=\"compose\")\n",
    "    \n",
    "    # Add audio\n",
    "    final_video = final_video.set_audio(audio_clip)\n",
    "    \n",
    "    # Set video properties\n",
    "    final_video = final_video.resize(height=720)  # HD resolution\n",
    "    final_video = final_video.set_fps(24)\n",
    "    \n",
    "    # Export video\n",
    "    video_path = VIDEOS_DIR / f\"{story_id}_video.mp4\"\n",
    "    \n",
    "    final_video.write_videofile(\n",
    "        str(video_path),\n",
    "        codec='libx264',\n",
    "        audio_codec='aac',\n",
    "        temp_audiofile='temp-audio.m4a',\n",
    "        remove_temp=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Generated video: {video_path}\")\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the story video\n",
    "print(\"Creating story video with effects...\")\n",
    "video_path = create_story_video(image_paths, audio_path, story_data['id'])\n",
    "\n",
    "if video_path:\n",
    "    print(f\"\\nStory video created successfully: {video_path}\")\n",
    "    print(f\"You can now use this video in your web application!\")\n",
    "else:\n",
    "    print(\"Failed to create video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subtitles(story_content, audio_duration):\n",
    "    \"\"\"Generate subtitle timing for the story\"\"\"\n",
    "    sentences = story_content.replace('\\n', ' ').split('. ')\n",
    "    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n",
    "    \n",
    "    duration_per_sentence = audio_duration / len(sentences)\n",
    "    \n",
    "    subtitles = []\n",
    "    current_time = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        subtitle = {\n",
    "            \"start\": round(current_time, 1),\n",
    "            \"end\": round(current_time + duration_per_sentence, 1),\n",
    "            \"text\": sentence\n",
    "        }\n",
    "        subtitles.append(subtitle)\n",
    "        current_time += duration_per_sentence\n",
    "    \n",
    "    return subtitles\n",
    "\n",
    "# Generate subtitles\n",
    "from moviepy.editor import AudioFileClip\n",
    "audio_clip = AudioFileClip(str(audio_path))\n",
    "subtitles = generate_subtitles(story_data['content'], audio_clip.duration)\n",
    "\n",
    "print(\"Generated subtitles:\")\n",
    "for subtitle in subtitles:\n",
    "    print(f\"{subtitle['start']}s - {subtitle['end']}s: {subtitle['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data for Web Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON data for web application\n",
    "web_story_data = {\n",
    "    \"id\": story_data['id'],\n",
    "    \"title\": story_data['title'],\n",
    "    \"content\": story_data['content'],\n",
    "    \"images\": [f\"/generated_content/images/{Path(p).name}\" for p in image_paths],\n",
    "    \"audioUrl\": f\"/generated_content/audio/{audio_path.name}\",\n",
    "    \"videoUrl\": f\"/generated_content/videos/{video_path.name}\" if video_path else None,\n",
    "    \"subtitles\": subtitles\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_json = OUTPUT_DIR / f\"{story_data['id']}_data.json\"\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(web_story_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nWeb application data saved to: {output_json}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(f\"- Images: {len(image_paths)} files in {IMAGES_DIR}\")\n",
    "print(f\"- Audio: {audio_path}\")\n",
    "if video_path:\n",
    "    print(f\"- Video: {video_path}\")\n",
    "print(f\"- Data: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process All Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process all 10 stories\n",
    "def process_all_stories():\n",
    "    \"\"\"Process all stories in the dataset\"\"\"\n",
    "    \n",
    "    # You can define all 10 stories here or load from external file\n",
    "    stories = [\n",
    "        # Add all your story data here\n",
    "        story_data  # We'll use the sample story for demo\n",
    "    ]\n",
    "    \n",
    "    processed_stories = []\n",
    "    \n",
    "    for story in stories:\n",
    "        print(f\"\\n=== Processing Story: {story['title']} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Generate images\n",
    "            image_paths = generate_story_images(story)\n",
    "            \n",
    "            # Generate audio\n",
    "            narration_text = story[\"content\"].replace('\\n', ' ').strip()\n",
    "            audio_path = generate_audio_gtts(narration_text, f\"{story['id']}_narration\")\n",
    "            \n",
    "            # Generate video\n",
    "            video_path = create_story_video(image_paths, audio_path, story['id'])\n",
    "            \n",
    "            # Generate subtitles\n",
    "            audio_clip = AudioFileClip(str(audio_path))\n",
    "            subtitles = generate_subtitles(story['content'], audio_clip.duration)\n",
    "            \n",
    "            # Create web data\n",
    "            web_data = {\n",
    "                \"id\": story['id'],\n",
    "                \"title\": story['title'],\n",
    "                \"content\": story['content'],\n",
    "                \"images\": [f\"/generated_content/images/{Path(p).name}\" for p in image_paths],\n",
    "                \"audioUrl\": f\"/generated_content/audio/{audio_path.name}\",\n",
    "                \"videoUrl\": f\"/generated_content/videos/{video_path.name}\" if video_path else None,\n",
    "                \"subtitles\": subtitles\n",
    "            }\n",
    "            \n",
    "            processed_stories.append(web_data)\n",
    "            print(f\"‚úÖ Successfully processed: {story['title']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {story['title']}: {str(e)}\")\n",
    "    \n",
    "    # Save all processed stories\n",
    "    all_stories_file = OUTPUT_DIR / \"all_stories_data.json\"\n",
    "    with open(all_stories_file, 'w') as f:\n",
    "        json.dump(processed_stories, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüéâ Batch processing complete! Data saved to: {all_stories_file}\")\n",
    "    return processed_stories\n",
    "\n",
    "# Uncomment to process all stories\n",
    "# processed_stories = process_all_stories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for Integration\n",
    "\n",
    "1. **Copy generated files** to your web project's `public/generated_content/` folder\n",
    "2. **Update story data** in your React app with the generated JSON\n",
    "3. **Test the video playback** in your web application\n",
    "4. **Adjust timing and effects** as needed for better user experience\n",
    "\n",
    "### File Structure for Web App:\n",
    "```\n",
    "public/\n",
    "‚îú‚îÄ‚îÄ generated_content/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wisdom-1_scene_1.png\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wisdom-1_scene_2.png\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ audio/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wisdom-1_narration.mp3\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ videos/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ wisdom-1_video.mp4\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}