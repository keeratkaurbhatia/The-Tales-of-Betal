# Hailuo integration with daily quota and HF fallback
# Env/config
HAILUO_API_KEY = os.getenv("HAILUO_API_KEY", "")
HAILUO_T2V_URL = os.getenv("HAILUO_T2V_URL", "")  # Provide your Hailuo direct T2V endpoint
HAILUO_DAILY_MAX = int(os.getenv("HAILUO_DAILY_MAX", "3"))

# Local usage tracking
META_DIR = (OUTPUT_DIR / 'meta') if 'OUTPUT_DIR' in globals() else Path('generated_content/meta')
META_DIR.mkdir(parents=True, exist_ok=True)
HAILUO_USAGE_FILE = META_DIR / 'hailuo_usage.json'

def _read_hailuo_usage():
    if HAILUO_USAGE_FILE.exists():
        try:
            with open(HAILUO_USAGE_FILE, 'r') as f:
                return json.load(f)
        except Exception:
            pass
    return {"date": "", "count": 0}

def _write_hailuo_usage(data):
    try:
        with open(HAILUO_USAGE_FILE, 'w') as f:
            json.dump(data, f)
    except Exception as e:
        print('Failed to write Hailuo usage file:', e)

def _can_use_hailuo_today() -> bool:
    data = _read_hailuo_usage()
    today = datetime.date.today().isoformat()
    if data.get('date') != today:
        data = {"date": today, "count": 0}
        _write_hailuo_usage(data)
    return int(data.get('count', 0)) < HAILUO_DAILY_MAX

def _record_hailuo_usage_increment():
    data = _read_hailuo_usage()
    today = datetime.date.today().isoformat()
    if data.get('date') != today:
        data = {"date": today, "count": 0}
    data['count'] = int(data.get('count', 0)) + 1
    _write_hailuo_usage(data)

# Hailuo request wrapper
from pathlib import Path as _PathAlias

def generate_video_clip_hailuo(prompt: str, filename: str, fps: int = 24, width: int = 512, height: int = 288) -> _PathAlias:
    if not HAILUO_API_KEY or not HAILUO_T2V_URL:
        raise RuntimeError('Hailuo not configured (missing HAILUO_API_KEY or HAILUO_T2V_URL)')
    headers = {
        'Authorization': f'Bearer {HAILUO_API_KEY}',
        'Accept': 'video/mp4',
        'Content-Type': 'application/json'
    }
    payload = {
        'prompt': f"{prompt}, cinematic, vibrant colors, detailed",
        'fps': fps,
        'width': width,
        'height': height
    }
    clips_dir = VIDEOS_DIR / 'clips'
    clips_dir.mkdir(exist_ok=True)
    out_path = clips_dir / f'{filename}.mp4'
    resp = requests.post(HAILUO_T2V_URL, headers=headers, json=payload, timeout=600)
    # If service returns binary mp4 directly
    if resp.status_code == 200 and resp.headers.get('Content-Type', '').startswith('video'):
        with open(out_path, 'wb') as f:
            f.write(resp.content)
        return out_path
    # Otherwise raise and let fallback handle
    raise RuntimeError(f'Hailuo T2V error {resp.status_code}: {resp.text[:200]}')

# Provider-preferring scene generator (Hailuo first, then HF)

def generate_scene_clips_from_prompts(story: dict, fps: int = 24, seconds_per_scene: float = 4.0) -> List[str]:
    clip_paths: List[str] = []
    num_frames = int(fps * seconds_per_scene)
    for i, scene_prompt in enumerate(story.get('scenes', []), start=1):
        fname = f"{story['id']}_scene_{i:02d}"
        used_hailuo = False
        if _can_use_hailuo_today():
            try:
                p = generate_video_clip_hailuo(scene_prompt, fname, fps=fps, width=512, height=288)
                _record_hailuo_usage_increment()
                used_hailuo = True
            except Exception as e:
                print('Hailuo generation failed or not available, falling back to HF:', e)
        if not used_hailuo:
            p = generate_video_clip_hf(scene_prompt, fname, num_frames=num_frames, fps=fps)
        clip_paths.append(str(p))
        time.sleep(2)
    return clip_paths
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Story Generator\n\nThis notebook generates a narrated video from a short story by:\n- Generating narration audio (gTTS)\n- Generating scene clips (Hugging Face T2V API)\n- Stitching with transitions and audio (moviepy)\n- Exporting a small JSON payload for a web client\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
                "# Setup\nimport os, json, time, requests, datetime\nfrom pathlib import Path\nfrom typing import List, Optional\nfrom moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\nfrom gtts import gTTS\n\n# Config\nHUGGINGFACE_TOKEN = \"your_huggingface_token_here\"\nOUTPUT_DIR = Path('generated_content')\nAUDIO_DIR = OUTPUT_DIR / 'audio'\nVIDEOS_DIR = OUTPUT_DIR / 'videos'\nfor d in [OUTPUT_DIR, AUDIO_DIR, VIDEOS_DIR, VIDEOS_DIR / 'clips']:\n    d.mkdir(parents=True, exist_ok=True)\n\n# Story data\nstory_data = {\n    'id': 'wisdom-1',\n    'title': 'The Wise Old Man and the Three Sons',\n    'content': \"\"\"Once upon a time, in a small village, there lived an old man with three sons.\nThe old man was known throughout the village for his wisdom and kindness.\nAs he grew older, he wanted to test which of his sons would inherit his wisdom.\nHe gave each son a single grain of rice and told them to make it multiply by the next full moon.\nThe first son planted it and got a small harvest.\nThe second son sold it and bought more rice.\nBut the third son gave it to a hungry child, saying that kindness multiplies in ways grain cannot.\"\"\",\n    'scenes': [\n        'An old wise man in traditional Indian clothing in a village setting',\n        'Three sons receiving grains of rice from their father',\n        'First son planting rice in a field, traditional Indian farming',\n        'Second son at a marketplace exchanging rice for money',\n        'Third son giving rice to a hungry child, showing compassion',\n        'The wise old man smiling, understanding true wisdom'\n    ]\n}\n\n# TTS (male Indian accent via gTTS + slight pitch drop)\ndef generate_audio_gtts(text: str, filename: str, lang: str = 'en', tld: str = 'co.in') -> Path:\n    out_path = AUDIO_DIR / f'{filename}.mp3'\n    tts = gTTS(text=text, lang=lang, tld=tld, slow=True)\n    tts.save(str(out_path))\n    try:\n        from pydub import AudioSegment\n        from pydub.effects import normalize\n        audio = AudioSegment.from_mp3(str(out_path))\n        audio = audio._spawn(audio.raw_data, overrides={'frame_rate': int(audio.frame_rate * 0.9)})\n        audio = audio.set_frame_rate(44100)\n        audio = normalize(audio)\n        audio.export(str(out_path), format='mp3')\n    except Exception as e:\n        print('pydub post-processing skipped or failed:', e)\n    return out_path\n\n# Direct Text-to-Video API\nHF_T2V_MODEL = 'damo-vilab/text-to-video-ms-1.7b'\nHF_T2V_URL = f'https://api-inference.huggingface.co/models/{HF_T2V_MODEL}'\n\ndef generate_video_clip_hf(prompt: str, filename: str, num_frames: int = 48, fps: int = 24, width: int = 512, height: int = 288) -> Path:\n    headers = {'Authorization': f'Bearer {HUGGINGFACE_TOKEN}', 'Accept': 'video/mp4'}\n    payload = {'inputs': f\"{prompt}, cinematic, Indian cultural aesthetics, vibrant colors, detailed\", 'parameters': {'num_frames': num_frames, 'fps': fps, 'width': width, 'height': height}}\n    clips_dir = VIDEOS_DIR / 'clips'\n    clips_dir.mkdir(exist_ok=True)\n    out_path = clips_dir / f'{filename}.mp4'\n    resp = requests.post(HF_T2V_URL, headers=headers, json=payload, timeout=600)\n    if resp.status_code == 200:\n        with open(out_path, 'wb') as f:\n            f.write(resp.content)\n        return out_path\n    else:\n        raise RuntimeError(f'HF T2V error {resp.status_code}: {resp.text[:200]}')\n\n\ndef generate_scene_clips_from_prompts(story: dict, fps: int = 24, seconds_per_scene: float = 4.0) -> List[str]:\n    clip_paths: List[str] = []\n    num_frames = int(fps * seconds_per_scene)\n    for i, scene_prompt in enumerate(story.get('scenes', []), start=1):\n        fname = f\"{story['id']}_scene_{i:02d}\"\n        p = generate_video_clip_hf(scene_prompt, fname, num_frames=num_frames, fps=fps)\n        clip_paths.append(str(p))\n        time.sleep(2)\n    return clip_paths\n\n# Stitching with audio\ndef stitch_clips_with_audio(clip_paths: List[str], audio_path: Path, story_id: str) -> Path:\n    audio_clip = AudioFileClip(str(audio_path))\n    total_duration = max(1.0, audio_clip.duration)\n    if not clip_paths:\n        raise ValueError('No clips to stitch')\n    duration_per_clip = total_duration / len(clip_paths)\n    transition = 0.4\n    clips = []\n    for i, p in enumerate(clip_paths):\n        c = VideoFileClip(p)\n        c = c.set_duration(duration_per_clip + (transition if i < len(clip_paths)-1 else 0))\n        if i > 0:\n            c = c.crossfadein(transition)\n        clips.append(c)\n    final = concatenate_videoclips(clips, method='compose')\n    final = final.set_duration(total_duration).set_audio(audio_clip).set_fps(24)\n    out_path = VIDEOS_DIR / f'{story_id}_video.mp4'\n    final.write_videofile(str(out_path), codec='libx264', audio_codec='aac', fps=24, verbose=False, logger=None)\n    return out_path\n\n# Subtitles\ndef generate_subtitles(story_content: str, audio_duration: float):\n    sentences = story_content.replace('\\n', ' ').split('. ')\n    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n    if not sentences:\n        return []\n    dur_per = audio_duration / len(sentences)\n    subs, t = [], 0.0\n    for s in sentences:\n        subs.append({'start': round(t, 2), 'end': round(t + dur_per, 2), 'text': s})\n        t += dur_per\n    return subs\n\n# Run pipeline\nnarration_text = story_data['content'].replace('\\n', ' ').strip()\naudio_filename = f\"{story_data['id']}_narration\"\naudio_path = generate_audio_gtts(narration_text, audio_filename)\nclip_paths = generate_scene_clips_from_prompts(story_data, fps=24, seconds_per_scene=4.0)\nvideo_path = stitch_clips_with_audio(clip_paths, audio_path, story_data['id'])\n\n# Export JSON\naudio_clip = AudioFileClip(str(audio_path))\nsubtitles = generate_subtitles(story_data['content'], audio_clip.duration)\nweb_story_data = {\n    'id': story_data['id'],\n    'title': story_data['title'],\n    'content': story_data['content'],\n    'images': [],\n    'audioUrl': f\"/generated_content/audio/{Path(audio_path).name}\",\n    'videoUrl': f\"/generated_content/videos/{Path(video_path).name}\",\n    'subtitles': subtitles\n}\nout_json = OUTPUT_DIR / f\"{story_data['id']}_data.json\"\nwith open(out_json, 'w') as f:\n    json.dump(web_story_data, f, indent=2)\nprint('Done:', video_path, audio_path, out_json)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
