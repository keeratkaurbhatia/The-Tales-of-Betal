{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smart Cultural Storyteller - AI Content Generation\n",
    "\n",
    "This notebook generates images and videos for the Betal storytelling project using free AI models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install requests pillow opencv-python moviepy huggingface-hub transformers torch diffusers\n",
    "!pip install elevenlabs gTTS pydub  # For text-to-speech with audio processing\n",
    "!pip install gradio  # For easy UI testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from PIL import Image, ImageEnhance, ImageFilter\n",
    "import cv2\n",
    "import numpy as np\n",
    "from moviepy.editor import *\n",
    "import torch\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from gtts import gTTS\n",
    "import io\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API Keys - Replace with your actual keys\n",
    "HUGGINGFACE_TOKEN = \"your_huggingface_token_here\"\n",
    "ELEVENLABS_API_KEY = \"your_elevenlabs_key_here\"  # Optional, can use gTTS instead\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path(\"generated_content\")\n",
    "IMAGES_DIR = OUTPUT_DIR / \"images\"\n",
    "AUDIO_DIR = OUTPUT_DIR / \"audio\"\n",
    "VIDEOS_DIR = OUTPUT_DIR / \"videos\"\n",
    "\n",
    "# Create directories\n",
    "for dir_path in [IMAGES_DIR, AUDIO_DIR, VIDEOS_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Story Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample story for generation\n",
    "story_data = {\n",
    "    \"id\": \"wisdom-1\",\n",
    "    \"title\": \"The Wise Old Man and the Three Sons\",\n",
    "    \"content\": \"\"\"Once upon a time, in a small village, there lived an old man with three sons. \n",
    "    The old man was known throughout the village for his wisdom and kindness. \n",
    "    As he grew older, he wanted to test which of his sons would inherit his wisdom.\n",
    "    He gave each son a single grain of rice and told them to make it multiply by the next full moon.\n",
    "    The first son planted it and got a small harvest. \n",
    "    The second son sold it and bought more rice. \n",
    "    But the third son gave it to a hungry child, saying that kindness multiplies in ways grain cannot.\"\"\",\n",
    "    \"scenes\": [\n",
    "        \"An old wise man in traditional Indian clothing in a village setting\",\n",
    "        \"Three sons receiving grains of rice from their father\",\n",
    "        \"First son planting rice in a field, traditional Indian farming\",\n",
    "        \"Second son at a marketplace exchanging rice for money\", \n",
    "        \"Third son giving rice to a hungry child, showing compassion\",\n",
    "        \"The wise old man smiling, understanding true wisdom\"\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Generation using Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Stable Diffusion pipeline (free model)\n",
    "def initialize_sd_pipeline():\n",
    "    model_id = \"runwayml/stable-diffusion-v1-5\"\n",
    "    pipe = StableDiffusionPipeline.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "    \n",
    "    # Use CPU if no GPU available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    pipe = pipe.to(device)\n",
    "    \n",
    "    if device == \"cuda\":\n",
    "        pipe.enable_memory_efficient_attention()\n",
    "    \n",
    "    return pipe\n",
    "\n",
    "# Alternative: Use Hugging Face Inference API (free tier)\n",
    "def generate_image_hf_api(prompt, filename):\n",
    "    API_URL = \"https://api-inference.huggingface.co/models/runwayml/stable-diffusion-v1-5\"\n",
    "    headers = {\"Authorization\": f\"Bearer {HUGGINGFACE_TOKEN}\"}\n",
    "    \n",
    "    # Enhanced prompt for Indian cultural context\n",
    "    enhanced_prompt = f\"{prompt}, Indian art style, traditional, cultural, detailed, beautiful colors, cinematic lighting\"\n",
    "    \n",
    "    payload = {\n",
    "        \"inputs\": enhanced_prompt,\n",
    "        \"parameters\": {\n",
    "            \"guidance_scale\": 7.5,\n",
            \"num_inference_steps\": 30,\n",
            \"width\": 1024,\n",
            \"height\": 768\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(API_URL, headers=headers, json=payload)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        image_path = IMAGES_DIR / f\"{filename}.png\"\n",
    "        with open(image_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Generated image: {image_path}\")\n",
    "        return image_path\n",
    "    else:\n",
    "        print(f\"Error generating image: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate images for the story\n",
    "def generate_story_images(story_data):\n",
    "    image_paths = []\n",
    "    \n",
    "    for i, scene_prompt in enumerate(story_data[\"scenes\"]):\n",
    "        filename = f\"{story_data['id']}_scene_{i+1}\"\n",
    "        image_path = generate_image_hf_api(scene_prompt, filename)\n",
    "        \n",
    "        if image_path:\n",
    "            image_paths.append(str(image_path))\n",
    "        \n",
    "        # Add delay to avoid rate limiting\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "    \n",
    "    return image_paths\n",
    "\n",
    "# Generate images\n",
    "print(\"Generating story images...\")\n",
    "image_paths = generate_story_images(story_data)\n",
    "print(f\"Generated {len(image_paths)} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Generation (Text-to-Speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use gTTS (Free, supports Indian English accent)\n",
    "def generate_audio_gtts(text, filename, lang='en', tld='co.in'):\n",
    "    \"\"\"Generate audio using Google Text-to-Speech with Indian accent and male-sounding voice\"\"\"\n",
    "    # Use slower speech and specific settings for deeper, more masculine tone\n",
    "    tts = gTTS(text=text, lang=lang, tld=tld, slow=True)\n",
    "    audio_path = AUDIO_DIR / f\"{filename}.mp3\"\n",
    "    tts.save(str(audio_path))\n",
    "    \n",
    "    # Post-process audio to make it sound more masculine\n",
    "    try:\n",
    "        from pydub import AudioSegment\n",
    "        from pydub.effects import normalize\n",
    "        \n",
    "        # Load and process audio\n",
    "        audio = AudioSegment.from_mp3(str(audio_path))\n",
    "        \n",
    "        # Lower pitch slightly for more masculine sound\n",
    "        audio = audio._spawn(audio.raw_data, overrides={\"frame_rate\": int(audio.frame_rate * 0.9)})\n",
    "        audio = audio.set_frame_rate(audio.frame_rate)\n",
    "        \n",
    "        # Normalize and save\n",
    "        audio = normalize(audio)\n",
    "        audio.export(str(audio_path), format=\"mp3\")\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"pydub not available - using basic gTTS output\")\n",
    "    \n",
    "    print(f\"Generated audio: {audio_path}\")\n",
    "    return audio_path\n",
    "\n",
    "# Option 2: ElevenLabs API (Better quality, limited free tier)\n",
    "def generate_audio_elevenlabs(text, filename):\n",
     "    \"\"\"Generate audio using ElevenLabs API with Daniel's deep male voice\"\"\"\n",
     "    url = \"https://api.elevenlabs.io/v1/text-to-speech/onwK4e9ZLuTAKqWW03F9\"  # Daniel - mature male voice\n",
    "    \n",
    "    headers = {\n",
    "        \"Accept\": \"audio/mpeg\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"xi-api-key\": ELEVENLABS_API_KEY\n",
    "    }\n",
    "    \n",
    "    data = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\",\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.5,\n",
    "            \"similarity_boost\": 0.5\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, json=data, headers=headers)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        audio_path = AUDIO_DIR / f\"{filename}.mp3\"\n",
    "        with open(audio_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Generated audio: {audio_path}\")\n",
    "        return audio_path\n",
    "    else:\n",
    "        print(f\"Error generating audio: {response.status_code}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate narration audio\n",
    "narration_text = story_data[\"content\"].replace('\\n', ' ').strip()\n",
    "audio_filename = f\"{story_data['id']}_narration\"\n",
    "\n",
    "print(\"Generating narration audio...\")\n",
    "# Use gTTS for free option\n",
    "audio_path = generate_audio_gtts(narration_text, audio_filename)\n",
    "\n",
    "# Uncomment below to use ElevenLabs instead\n",
    "# audio_path = generate_audio_elevenlabs(narration_text, audio_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Generation with Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
def create_animated_clip(image_path, duration=4, clip_type=\"kenburns\"):\n",
    \"\"\"Create animated clip from single image with various effects\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    \n",
    "    # Resize image for animation effects\n",
    "    original_size = image.size\n",
    "    new_width = int(original_size[0] * 1.3)\n",
    "    new_height = int(original_size[1] * 1.3)\n",
    "    image = image.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "    \n",
    "    # Create base image clip\n",
    "    clip = ImageClip(np.array(image), duration=duration)\n",
    "    \n",
    "    if clip_type == \"kenburns\":\n",
    "        # Ken Burns effect: slow zoom and pan\n",
    "        clip = clip.resize(lambda t: 1 + 0.15 * t / duration)\n",
    "        # Add subtle pan movement\n",
    "        clip = clip.set_position(lambda t: (\n",
    "            'center' if t < duration/2 else ('left' if t % 2 == 0 else 'right'),\n",
    "            'center'\n",
    "        ))\n",
    "    \n",
    "    elif clip_type == \"parallax\":\n",
    "        # Parallax effect: layered movement\n",
    "        clip = clip.set_position(lambda t: (\n",
    "            int(20 * np.sin(t * 0.5)),  # Horizontal movement\n",
    "            int(10 * np.cos(t * 0.3))   # Vertical movement\n",
    "        ))\n",
    "    \n",
    "    elif clip_type == \"zoom_pan\":\n",
    "        # Zoom with directional pan\n",
    "        clip = clip.resize(lambda t: 1 + 0.2 * (t / duration))\n",
    "        clip = clip.set_position(lambda t: (\n",
    "            int(-30 + 60 * t / duration),  # Pan from left to right\n",
    "            'center'\n",
    "        ))\n",
    "    \n",
    "    # Ensure clip fits screen\n",
    "    clip = clip.resize(height=720)  # HD height\n",
    "    \n",
    "    return clip\n",
    "\n",
def add_mystical_effects(clip, effect_type=\"fade\"):\n",
    \"\"\"Add mystical visual effects to clips\"\"\"\n",
    "    # Add fade in/out\n",
    "    clip = clip.fadeout(0.8).fadein(0.8)\n",
    "    \n",
    "    if effect_type == \"mystical\":\n",
    "        # Add color enhancement for mystical feel\n",
    "        clip = clip.fx(afx.colorx, factor=1.15)\n",
    "        # Add slight blur for dreamy effect\n",
    "        # clip = clip.fx(vfx.blur, 1)\n",
    "    \n",
    "    elif effect_type == \"dramatic\":\n",
    "        # Higher contrast for dramatic scenes\n",
    "        clip = clip.fx(afx.colorx, factor=1.2)\n",
    "    \n",
    "    return clip\n",
    "\n",
def create_individual_clips(image_paths, story_id, clip_duration=4.0):\n",
    \"\"\"Create individual animated clips for each scene\"\"\"\n",
    "    clips_dir = VIDEOS_DIR / \"clips\"\n",
    "    clips_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    clip_paths = []\n",
    "    animation_types = [\"kenburns\", \"parallax\", \"zoom_pan\"]\n",
    "    effect_types = [\"mystical\", \"dramatic\", \"fade\"]\n",
    "    \n",
    "    for i, image_path in enumerate(image_paths):\n",
    "        if os.path.exists(image_path):\n",
    "            print(f\"Creating animated clip {i+1}/{len(image_paths)}...\")\n",
    "            \n",
    "            # Vary animation types for visual interest\n",
    "            animation_type = animation_types[i % len(animation_types)]\n",
    "            effect_type = effect_types[i % len(effect_types)]\n",
    "            \n",
    "            # Create animated clip\n",
    "            clip = create_animated_clip(image_path, clip_duration, animation_type)\n",
    "            \n",
    "            # Add mystical effects\n",
    "            clip = add_mystical_effects(clip, effect_type)\n",
    "            \n",
    "            # Export individual clip\n",
    "            clip_filename = f\"{story_id}_scene_{i+1:02d}.mp4\"\n",
    "            clip_path = clips_dir / clip_filename\n",
    "            \n",
    "            clip.write_videofile(\n",
    "                str(clip_path),\n",
    "                codec='libx264',\n",
    "                fps=24,\n",
    "                preset='medium',\n",
    "                verbose=False,\n",
    "                logger=None\n",
    "            )\n",
    "            \n",
    "            clip_paths.append(str(clip_path))\n",
    "            print(f\"‚úÖ Created clip: {clip_path}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Image not found: {image_path}\")\n",
    "    \n",
    "    return clip_paths\n",
    "\n",
def stitch_clips_with_audio(clip_paths, audio_path, story_id):\n",
    \"\"\"Stitch individual clips together with audio and transitions\"\"\"\n",
    "    \n",
    "    # Load audio to get duration\n",
    "    audio_clip = AudioFileClip(str(audio_path))\n",
    "    total_duration = audio_clip.duration\n",
    "    \n",
    "    print(f\"Audio duration: {total_duration:.2f} seconds\")\n",
    "    print(f\"Number of clips: {len(clip_paths)}\")\n",
    "    \n",
    "    # Calculate duration per image\n",
    "    duration_per_clip = total_duration / len(clip_paths)\n",
    "    transition_duration = 0.5  # Crossfade duration\n",
    "    \n",
    "    # Load and adjust video clips\n",
    "    video_clips = []\n",
    "    \n",
    "    for i, clip_path in enumerate(clip_paths):\n",
    "        if os.path.exists(clip_path):\n",
    "            # Load the pre-created clip\n",
    "            clip = VideoFileClip(clip_path)\n",
    "            \n",
    "            # Adjust duration to match audio timing\n",
    "            clip = clip.set_duration(duration_per_clip + transition_duration)\n",
    "            \n",
    "            # Set start time with overlap for transitions\n",
    "            start_time = i * (duration_per_clip - transition_duration/2) if i > 0 else 0\n",
    "            clip = clip.set_start(start_time)\n",
    "            \n",
    "            # Add crossfade transition (except for first clip)\n",
    "            if i > 0:\n",
    "                clip = clip.crossfadein(transition_duration)\n",
    "            \n",
    "            video_clips.append(clip)\n",
    "            print(f\"Added clip {i+1}: duration={duration_per_clip:.2f}s, start={start_time:.2f}s\")\n",
    "        else:\n",
    "            print(f\"Clip not found: {clip_path}\")\n",
    "    \n",
    "    if not video_clips:\n",
    "        print(\"No valid clips found for video creation\")\n",
    "        return None\n",
    "    \n",
    "    # Composite video clips with transitions\n",
    "    final_video = concatenate_videoclips(video_clips, method=\"compose\")\n",
    "    \n",
    "    # Ensure video matches audio duration\n",
    "    final_video = final_video.set_duration(total_duration)\n",
    "    \n",
    "    # Add audio\n",
    "    final_video = final_video.set_audio(audio_clip)\n",
    "    \n",
    "    # Set video properties\n",
    "    final_video = final_video.resize(height=720).resize(width=1280)  # HD resolution\n",
    "    final_video = final_video.set_fps(24)\n",
    "    \n",
    "    # Export video\n",
    "    video_path = VIDEOS_DIR / f\"{story_id}_video.mp4\"\n",
    "    \n",
    "    final_video.write_videofile(\n",
    "        str(video_path),\n",
    "        codec='libx264',\n",
    "        audio_codec='aac',\n",
    "        preset='medium',\n",
    "        fps=24,\n",
    "        verbose=False,\n",
    "        logger=None\n",
    "    )\n",
    "    \n",
    "    # Clean up individual clips to save space\n",
    "    # for clip_path in clip_paths:\n",
    "    #     if os.path.exists(clip_path):\n",
    "    #         os.remove(clip_path)\n",
    "    \n",
    "    print(f\"Generated video: {video_path}\")\n",
    "    return video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the story video\n",
    "print(\"Creating individual animated clips...\")\n",
    "clip_paths = create_individual_clips(image_paths, story_data['id'], clip_duration=4.0)\n",
    "\n",
    "print(\"\\nStitching clips together with audio...\")\n",
    "video_path = stitch_clips_with_audio(clip_paths, audio_path, story_data['id'])\n",
    "\n",
    "if video_path:\n",
    "    print(f\"\\nüé¨ Story video created successfully: {video_path}\")\n",
    "    print(f\"üìÅ Individual clips saved in: {VIDEOS_DIR / 'clips'}\")\n",
    "    print(f\"You can now use this video in your web application!\")\n",
    "else:\n",
    "    print(\"Failed to create video\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_subtitles(story_content, audio_duration):\n",
    "    \"\"\"Generate subtitle timing for the story\"\"\"\n",
    "    sentences = story_content.replace('\\n', ' ').split('. ')\n",
    "    sentences = [s.strip() + '.' for s in sentences if s.strip()]\n",
    "    \n",
    "    duration_per_sentence = audio_duration / len(sentences)\n",
    "    \n",
    "    subtitles = []\n",
    "    current_time = 0\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        subtitle = {\n",
    "            \"start\": round(current_time, 1),\n",
    "            \"end\": round(current_time + duration_per_sentence, 1),\n",
    "            \"text\": sentence\n",
    "        }\n",
    "        subtitles.append(subtitle)\n",
    "        current_time += duration_per_sentence\n",
    "    \n",
    "    return subtitles\n",
    "\n",
    "# Generate subtitles\n",
    "from moviepy.editor import AudioFileClip\n",
    "audio_clip = AudioFileClip(str(audio_path))\n",
    "subtitles = generate_subtitles(story_data['content'], audio_clip.duration)\n",
    "\n",
    "print(\"Generated subtitles:\")\n",
    "for subtitle in subtitles:\n",
    "    print(f\"{subtitle['start']}s - {subtitle['end']}s: {subtitle['text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Data for Web Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create JSON data for web application\n",
    "web_story_data = {\n",
    "    \"id\": story_data['id'],\n",
    "    \"title\": story_data['title'],\n",
    "    \"content\": story_data['content'],\n",
    "    \"images\": [f\"/generated_content/images/{Path(p).name}\" for p in image_paths],\n",
    "    \"audioUrl\": f\"/generated_content/audio/{audio_path.name}\",\n",
    "    \"videoUrl\": f\"/generated_content/videos/{video_path.name}\" if video_path else None,\n",
    "    \"subtitles\": subtitles\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "output_json = OUTPUT_DIR / f\"{story_data['id']}_data.json\"\n",
    "with open(output_json, 'w') as f:\n",
    "    json.dump(web_story_data, f, indent=2)\n",
    "\n",
    "print(f\"\\nWeb application data saved to: {output_json}\")\n",
    "print(\"\\nGenerated files:\")\n",
    "print(f\"- Images: {len(image_paths)} files in {IMAGES_DIR}\")\n",
    "print(f\"- Audio: {audio_path}\")\n",
    "if video_path:\n",
    "    print(f\"- Video: {video_path}\")\n",
    "print(f\"- Data: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Process All Stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process all 10 stories\n",
    "def process_all_stories():\n",
    "    \"\"\"Process all stories in the dataset\"\"\"\n",
    "    \n",
    "    # You can define all 10 stories here or load from external file\n",
    "    stories = [\n",
    "        # Add all your story data here\n",
    "        story_data  # We'll use the sample story for demo\n",
    "    ]\n",
    "    \n",
    "    processed_stories = []\n",
    "    \n",
    "    for story in stories:\n",
    "        print(f\"\\n=== Processing Story: {story['title']} ===\")\n",
    "        \n",
    "        try:\n",
    "            # Generate images\n",
    "            image_paths = generate_story_images(story)\n",
    "            \n",
    "            # Generate audio\n",
    "            narration_text = story[\"content\"].replace('\\n', ' ').strip()\n",
    "            audio_path = generate_audio_gtts(narration_text, f\"{story['id']}_narration\")\n",
    "            \n",
    "            # Create individual animated clips\n",
    "            clip_paths = create_individual_clips(image_paths, story['id'], clip_duration=4.0)\n",
    "            \n",
    "            # Stitch clips with audio\n",
    "            video_path = stitch_clips_with_audio(clip_paths, audio_path, story['id'])\n",
    "            \n",
    "            # Generate subtitles\n",
    "            audio_clip = AudioFileClip(str(audio_path))\n",
    "            subtitles = generate_subtitles(story['content'], audio_clip.duration)\n",
    "            \n",
    "            # Create web data\n",
    "            web_data = {\n",
    "                \"id\": story['id'],\n",
    "                \"title\": story['title'],\n",
    "                \"content\": story['content'],\n",
    "                \"images\": [f\"/generated_content/images/{Path(p).name}\" for p in image_paths],\n",
    "                \"audioUrl\": f\"/generated_content/audio/{audio_path.name}\",\n",
    "                \"videoUrl\": f\"/generated_content/videos/{video_path.name}\" if video_path else None,\n",
    "                \"subtitles\": subtitles\n",
    "            }\n",
    "            \n",
    "            processed_stories.append(web_data)\n",
    "            print(f\"‚úÖ Successfully processed: {story['title']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error processing {story['title']}: {str(e)}\")\n",
    "    \n",
    "    # Save all processed stories\n",
    "    all_stories_file = OUTPUT_DIR / \"all_stories_data.json\"\n",
    "    with open(all_stories_file, 'w') as f:\n",
    "        json.dump(processed_stories, f, indent=2)\n",
    "    \n",
    "    print(f\"\\nüéâ Batch processing complete! Data saved to: {all_stories_file}\")\n",
    "    return processed_stories\n",
    "\n",
    "# Uncomment to process all stories\n",
    "# processed_stories = process_all_stories()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions for Integration\n",
    "\n",
    "1. **Copy generated files** to your web project's `public/generated_content/` folder\n",
    "2. **Update story data** in your React app with the generated JSON\n",
    "3. **Test the video playback** in your web application\n",
    "4. **Adjust timing and effects** as needed for better user experience\n",
    "\n",
    "### File Structure for Web App:\n",
    "```\n",
    "public/\n",
    "‚îú‚îÄ‚îÄ generated_content/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ images/\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wisdom-1_scene_1.png\n",
    "‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ wisdom-1_scene_2.png\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ audio/\n",
    "‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ wisdom-1_narration.mp3\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ videos/\n",
    "‚îÇ       ‚îî‚îÄ‚îÄ wisdom-1_video.mp4\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}